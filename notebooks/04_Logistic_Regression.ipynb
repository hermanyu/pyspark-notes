{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5bc90a6-6909-4d9a-afcb-638ed7efc807",
   "metadata": {},
   "source": [
    "# 04 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f27dfa-ffd4-4b36-bed8-a2e77fece6e3",
   "metadata": {},
   "source": [
    "## 4.1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d77d91c-e41d-4624-8108-77dfe8c159c6",
   "metadata": {},
   "source": [
    "Logistic regression is a *generalized linear model (GLM)* used for estimating the probability of an event occuring. Formally, let $Y$ be a Bernoulli random variable. That is, $Y = 1$ if event $E$ occurs while $Y = 0$ if event $E$ does not occur. What we wish to do is estimate the probability $P(Y = 1)$ using some predictor variables $X_1, X_2, \\ldots, X_m$. Mathematically, this simply means we wish to estimate the conditional probability:\n",
    "\n",
    "$$\n",
    "P(Y = 1 | X_1, \\ldots, X_m)\n",
    "$$\n",
    "\n",
    "Logistic regression attempts to estimate this probability by taking a linear model $\\beta X := \\beta_0 + \\beta_1X_1 + \\ldots + \\beta_mX_m$ and feeding it through a **logistic function** (aka a \"sigmoid\" function):\n",
    "\n",
    "$$\n",
    "P(Y = 1 | X_1, \\ldots, X_m) = \\frac{1}{1 + e^{\\beta X}}\n",
    "$$\n",
    "\n",
    "The inverse of the logistic function is the **logit function** $\\text{logit}(p) = \\ln\\frac{p}{1-p}$, which we can use to isolate the \"linearity\" in the expression above:\n",
    "\n",
    "$$\n",
    "\\text{logit}P(Y = 1 | X_1, \\ldots, X_m) = \\beta X = \\beta_0 + \\beta_1X_1 + \\ldots + \\beta_mX_m\n",
    "$$\n",
    "\n",
    "Thus, a logistic regression is really just a linear regression applied to the quantity $\\text{logit}P(Y = 1 | X_1, \\ldots, X_m)$. For this reason, logistic regression is called a **generalized linear model** with $\\text{logit}(p)$ called the **link function**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636dd071-f32b-4e96-8ff0-e7de17c78913",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72fd1ac-ee56-4a00-87e8-ea1d338cc362",
   "metadata": {},
   "source": [
    "## 4.2 The Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dd6775be-76c9-4773-bbf0-34adce28cd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "SEED = 42\n",
    "DATA_PATH = \"../course_materials/Spark_for_Machine_Learning/Logistic_Regression/\"\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Logistic Regression\").getOrCreate()\n",
    "\n",
    "df_sample = spark.read.format(\"libsvm\").load(DATA_PATH + \"sample_libsvm_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49f5c88d-6945-4251-b350-0fa8fd87903e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. rows: 100\n",
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(692,[127,128,129...|\n",
      "|  1.0|(692,[158,159,160...|\n",
      "|  1.0|(692,[124,125,126...|\n",
      "|  1.0|(692,[152,153,154...|\n",
      "|  1.0|(692,[151,152,153...|\n",
      "|  0.0|(692,[129,130,131...|\n",
      "|  1.0|(692,[158,159,160...|\n",
      "|  1.0|(692,[99,100,101,...|\n",
      "|  0.0|(692,[154,155,156...|\n",
      "|  0.0|(692,[127,128,129...|\n",
      "+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Num. rows: {df_sample.count()}\")\n",
    "df_sample.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efcec81c-57fe-4f2a-8ba5-fa0f0b8eb4d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(692, {127: 51.0, 128: 159.0, 129: 253.0, 130: 159.0, 131: 50.0, 154: 48.0, 155: 238.0, 156: 252.0, 157: 252.0, 158: 252.0, 159: 237.0, 181: 54.0, 182: 227.0, 183: 253.0, 184: 252.0, 185: 239.0, 186: 233.0, 187: 252.0, 188: 57.0, 189: 6.0, 207: 10.0, 208: 60.0, 209: 224.0, 210: 252.0, 211: 253.0, 212: 252.0, 213: 202.0, 214: 84.0, 215: 252.0, 216: 253.0, 217: 122.0, 235: 163.0, 236: 252.0, 237: 252.0, 238: 252.0, 239: 253.0, 240: 252.0, 241: 252.0, 242: 96.0, 243: 189.0, 244: 253.0, 245: 167.0, 262: 51.0, 263: 238.0, 264: 253.0, 265: 253.0, 266: 190.0, 267: 114.0, 268: 253.0, 269: 228.0, 270: 47.0, 271: 79.0, 272: 255.0, 273: 168.0, 289: 48.0, 290: 238.0, 291: 252.0, 292: 252.0, 293: 179.0, 294: 12.0, 295: 75.0, 296: 121.0, 297: 21.0, 300: 253.0, 301: 243.0, 302: 50.0, 316: 38.0, 317: 165.0, 318: 253.0, 319: 233.0, 320: 208.0, 321: 84.0, 328: 253.0, 329: 252.0, 330: 165.0, 343: 7.0, 344: 178.0, 345: 252.0, 346: 240.0, 347: 71.0, 348: 19.0, 349: 28.0, 356: 253.0, 357: 252.0, 358: 195.0, 371: 57.0, 372: 252.0, 373: 252.0, 374: 63.0, 384: 253.0, 385: 252.0, 386: 195.0, 399: 198.0, 400: 253.0, 401: 190.0, 412: 255.0, 413: 253.0, 414: 196.0, 426: 76.0, 427: 246.0, 428: 252.0, 429: 112.0, 440: 253.0, 441: 252.0, 442: 148.0, 454: 85.0, 455: 252.0, 456: 230.0, 457: 25.0, 466: 7.0, 467: 135.0, 468: 253.0, 469: 186.0, 470: 12.0, 482: 85.0, 483: 252.0, 484: 223.0, 493: 7.0, 494: 131.0, 495: 252.0, 496: 225.0, 497: 71.0, 510: 85.0, 511: 252.0, 512: 145.0, 520: 48.0, 521: 165.0, 522: 252.0, 523: 173.0, 538: 86.0, 539: 253.0, 540: 225.0, 547: 114.0, 548: 238.0, 549: 253.0, 550: 162.0, 566: 85.0, 567: 252.0, 568: 249.0, 569: 146.0, 570: 48.0, 571: 29.0, 572: 85.0, 573: 178.0, 574: 225.0, 575: 253.0, 576: 223.0, 577: 167.0, 578: 56.0, 594: 85.0, 595: 252.0, 596: 252.0, 597: 252.0, 598: 229.0, 599: 215.0, 600: 252.0, 601: 252.0, 602: 252.0, 603: 196.0, 604: 130.0, 622: 28.0, 623: 199.0, 624: 252.0, 625: 252.0, 626: 253.0, 627: 252.0, 628: 252.0, 629: 233.0, 630: 145.0, 651: 25.0, 652: 128.0, 653: 252.0, 654: 253.0, 655: 252.0, 656: 141.0, 657: 37.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.collect()[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f26a3d9-2f64-4916-8102-f015ba244f53",
   "metadata": {},
   "source": [
    "This dataframe contains only 100 rows, but what is interesting is the `features` column which is a 692-dimensional vector. This vector is a **sparse vector** meaning most of its entries are 0, and that the nonzero entries are stored as a dictionary of the form `{ index : value }`. These kinds of sparse vectors are very common for online customer behavior. For example, each entry could represent the amount a customer spent on a specific product on Amazon. Most customers will only ever purchase a very small subset of all the potential products for sale, so we would typically see a very sparse vector such as this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "427337f5-89d5-4e31-b585-e76229cb3025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  1.0|   57|\n",
      "|  0.0|   43|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sample.withColumn(\"label\", df_sample[\"label\"].cast('string')).groupBy(\"label\").agg(F.count(\"label\").alias(\"count\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf54a55-7906-4116-89cf-98201e842efe",
   "metadata": {},
   "source": [
    "The other important thing to note is that the `label` column is an indicator variable: 1 = positive class, 0 = negative class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a11007-8051-4038-a604-55dee52171cb",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c168d81-5fe9-4e6b-8e7c-1c47031dbacf",
   "metadata": {},
   "source": [
    "## 4.3 The `LogisticRegression` Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daece0a-13de-4b64-80f7-dcca208e7a5e",
   "metadata": {},
   "source": [
    "We wish to build a logistic regression model to try and correctly classify the rows of the dataframe as either `label = 1` or `label = 0`, using the `features` column as a predictor. To do this, we can use the `LogisticRegression` estimator found in the `pyspark.ml.classification` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0b6bc6e-b0f6-4dec-bb64-08c838eece49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# instantiate a new LogisticRegression Estimator\n",
    "logistic_estimator = LogisticRegression(\n",
    "    featuresCol = \"features\",\n",
    "    labelCol = \"label\",\n",
    "    predictionCol = \"prediction\"\n",
    ")\n",
    "\n",
    "# use the estimator to fit a model on the data\n",
    "logistic_model = logistic_estimator.fit(df_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "796ed4bf-a37a-425f-8f12-aecf9577c00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model Accuracy: {logistic_model.summary.accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a7312d-9c24-4e64-ae67-b974a93eb2d7",
   "metadata": {},
   "source": [
    "Notice that the model achieved 100% prediction accuracy on the data. This is actually a very bad sign because it usually indicates severe overfitting. Recall that for a predictive model, what we actually care about is accuracy on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70f147d-90e2-4876-bc10-229863c4cd33",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8c3879-b832-4ac6-aa70-9d62e66d981a",
   "metadata": {},
   "source": [
    "## 4.4 Train-Test Split and Evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bafd19c-3453-40cb-a93b-c791cb31a938",
   "metadata": {},
   "source": [
    "In order to get an estimate for how the model performs on unseen data, we should use a train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "45b1e6ea-f44f-4e49-bf68-ea6a89d31929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "df_train, df_test = df_sample.randomSplit([0.7, 0.3], seed = SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0c6bec1d-4fc5-4a94-aa0c-a28ad7c3c39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a new LogisticRegression Estimator\n",
    "logistic_estimator = LogisticRegression(\n",
    "    featuresCol = \"features\",\n",
    "    labelCol = \"label\",\n",
    "    predictionCol = \"prediction\"\n",
    ")\n",
    "\n",
    "# use the estimator to fit a model on the data\n",
    "logistic_model = logistic_estimator.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ee03183d-86d9-4c48-ad66-ccb619c7e0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 0.9714285714285714\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train Accuracy: {logistic_model.summary.accuracy}\")\n",
    "print(f\"Test Accuracy: {logistic_model.evaluate(df_test).accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c951eb4-3342-4428-950d-7b8cd8137329",
   "metadata": {},
   "source": [
    "Surprisingly we achieve very high training accuracy on testing data as well. This indicates that the underlying data we are predicting on is highly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77436110-367f-4219-b8d5-3018f899a71b",
   "metadata": {},
   "source": [
    "Accuracy isn't the only metric that matters when evaluating classification models. Other important metrics include:\n",
    "\n",
    "* Precision: if the model predicts \"positive\", how likely is it to be a true positive?\n",
    "* Recall / Sensitivity: amongst all the \"true positives\", how likely is the model to detect them?\n",
    "* Specificity: amonst all the \"true negatives\", how likely is the model to detect them?\n",
    "* Area Under the ROC Curve: this measures how well the model is able to balance between detecting true positives vs claiming a false positive\n",
    "\n",
    "All of these metrics can be easily computed using the `BinaryClassificationEvaluator` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0810ba67-1ed4-4178-9b65-66117ea4cfa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "logistic_eval = BinaryClassificationEvaluator(\n",
    "    labelCol = \"label\",\n",
    "    metricName = \"areaUnderROC\" # the AUC is the default metric\n",
    ")\n",
    "\n",
    "logistic_results = logistic_model.evaluate(df_test)\n",
    "\n",
    "logistic_eval.evaluate(logistic_results.predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2b0666-b472-4503-9230-08363bb059bb",
   "metadata": {},
   "source": [
    "The model achieves a perfect AUC of 1.0 on the testing data, which means the model is able to detect all the true positives without predicting any false positives. Real world data will almost never achieve such a perfect AUC of 1.0, so it's likely the data is just very easy to separate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e1908a-2773-4d83-84ce-7ded5e88acc2",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9fd4f0-191f-4c26-b531-051d8b798b68",
   "metadata": {},
   "source": [
    "## 4.5 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c877d94-dc11-4d2d-abe9-5a0788fec2a8",
   "metadata": {},
   "source": [
    "Let us pretend that our logistic regression model was, in fact, overfit. We could attempt to regularize it by adding an L1 and/or L2 penalty term to the loss function to restrict the possible range of fitted coefficient values, thereby restricting the complexity of the model. This can be achieved by using the built-in `regParam` and `elasticNetParam` arguments of the `LogisticRegression` estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "16a14384-1a21-4813-a48a-f38a12df523d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 0.9428571428571428\n"
     ]
    }
   ],
   "source": [
    "# instantiate a new LogisticRegression Estimator\n",
    "# but this time with regularization\n",
    "logistic_estimator = LogisticRegression(\n",
    "    featuresCol = \"features\",\n",
    "    labelCol = \"label\",\n",
    "    predictionCol = \"prediction\",\n",
    "    regParam = 0.1,     # weight of the penalty\n",
    "    elasticNetParam = 0 # L2 vs L1 penalty slider\n",
    "                        # 0 = L2 penalty only ; 1 = L1 penalty only\n",
    ")\n",
    "\n",
    "# use the estimator to fit a model on the data\n",
    "logistic_model = logistic_estimator.fit(df_train)\n",
    "\n",
    "print(f\"Train Accuracy: {logistic_model.summary.accuracy}\")\n",
    "print(f\"Test Accuracy: {logistic_model.evaluate(df_test).accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d920e3-0b02-4445-bf5d-3d4874d50ab7",
   "metadata": {},
   "source": [
    "Adding a penalty term actually makes the model fit worse on testing data, which again indicates that the underlying data is highly separable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5531cae6-7e8f-4a18-b00a-376f388f0c4d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf3e1c1-c9d2-400c-b559-1e5bffb10dc9",
   "metadata": {},
   "source": [
    "## 4.6 Hyperparameter Tuning with Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674c4283-e26f-4aa3-b6c7-8ae4b09517a2",
   "metadata": {},
   "source": [
    "In the previous section, we chose the weight of `regParam` somewhat arbitrarily. A better approach would be to systematically test different values for `regParam`, based on how the model preforms on the testing data with each specific choice of `regParam` value. This is called **hyperparameter tuning** and can be automated by searching through a set of candidate `regParam` values called the **hyperparameter grid**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35c013c-3c6b-423e-891d-51eed8eda507",
   "metadata": {},
   "source": [
    "If we were to just evaluate the different choices of `regParam` on our `df_test` data set, we run the risk of overfitting again because we might just happen to find a `regParam` value that works very well on `df_test` specifically, and not on unseen data. To mitigate this kind of issue, we typically want to test out our `regParam` values on a bunch of different versions of test data. This process is called **$k$-fold cross-validation** is done like so:\n",
    "\n",
    "1) Start with a specific train-test split of the data.\n",
    "2) Fit the model on the training data set and evaluate the accuracy on the test data.\n",
    "3) Shuffle the data around to create a new train-test split and go back to step 2.\n",
    "\n",
    "We go throug this loop $k$-many times for each candidate `regParam` value we want to test out. The average test accuracy of the $k$-many loops is computed and the optimal `regParam` value is the one with the highest average test accuracy. This process can be done by using the `ParamGridBuilder` and `CrossValidator` classes in `pyspark.ml.tuning` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "06df6f0b-8714-4bb5-b249-88472c9d6b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# instantiate new LogisticRegression estimator\n",
    "logistic_estimator = LogisticRegression(\n",
    "    featuresCol = \"features\",\n",
    "    labelCol = \"label\",\n",
    "    predictionCol = \"prediction\"\n",
    ")\n",
    "\n",
    "# build a hyperparameter grid for \n",
    "# the hyper parameter regParam\n",
    "param_grid = ParamGridBuilder()\\\n",
    "    .addGrid(logistic_estimator.regParam, [0.1, 0.01])\\\n",
    "    .build()\n",
    "\n",
    "# instantiate CrossValidator\n",
    "crossval = CrossValidator(\n",
    "    estimator = logistic_estimator,  # the Estimator to fit the models\n",
    "    estimatorParamMaps = param_grid, # the set of candidate hyperparam values\n",
    "    evaluator = BinaryClassificationEvaluator(), # specifies how to evaluate on testing data\n",
    "    numFolds = 3\n",
    ")\n",
    "\n",
    "# run 3-fold cross validation\n",
    "logistic_cv = crossval.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fc14ca7f-b102-4aab-bb2a-b37ee414f0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal regParam value = 0.1\n"
     ]
    }
   ],
   "source": [
    "optimal_regParam = logistic_cv.bestModel._java_obj.getRegParam()\n",
    "\n",
    "print(f\"Optimal regParam value = {optimal_regParam}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927d4646-25e2-4ee9-91a4-1e7b095b31af",
   "metadata": {},
   "source": [
    "Note: cross-validation can also be used to estimate generalization error, without the need to tune hyperparameters. We can always pass an empty `ParamGrid` if we just want to do CV without hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7981f344-c925-472b-9d4d-8948cfe20259",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
