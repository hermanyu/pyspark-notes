{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 Spark Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Setting Up A SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use PySpark, we must first set up a `SparkSession`. A `SparkSession` is an API through which Python code gets translated to Spark code. To start a `SparkSession`, we load in the `SparkSession` object from the `pyspark.sql` module and running some boilerplate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# build a SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark Basics\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Reading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a `SparkSession` has been set up, we can begin reading in data. This can be done in multiple different ways depending on the context of where we are running our code. For example:\n",
    "\n",
    "1) If we are running our code on a local machine, we can read in flat files like `.csv`, `.txt`, `.JSON`, etc. directly into memory.\n",
    "\n",
    "2) If we are on Databricks, we can query an existing table in the data warehouse using SQL syntax: `spark.sql(\"SELECT * FROM mytable\")`\n",
    "\n",
    "This notebook will load data from a local machine, which can be done by using the various `read.<file extension>` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_people = spark.read.json(\"../course_materials/Spark_Dataframes/people.json\")\n",
    "df_sales = spark.read.csv(\"../course_materials/Spark_Dataframes/sales_info.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to first read in a flat file as a Pandas dataframe, then convert it into a Spark dataframe. This can be done using the `createDataFrame()` method from the `SparkSession` object. This is convenient if Pandas has a specific functionality that we want to take advantage (such as inferring schema automatically from `.csv` files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read\n",
    "df_sales = pd.read_csv(\"../course_materials/Spark_Dataframes/sales_info.csv\")\n",
    "\n",
    "df_sales = spark.createDataFrame(df_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Person: string (nullable = true)\n",
      " |-- Sales: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Spark Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark will store data in so-called **Spark Dataframes**. Unlike PANDAS dataframes, operations on Spark dataframes are meant to be executed on distributed compute / compute clusters (e.g. Databricks) and are meant to be manipulated using the Spark language. Consequently, this means that Spark dataframes are not compatible with standard python functions like `print()`. Instead, we will generally need to methods specific to Spark dataframe objects in order to manipulate them correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[age: bigint, name: string]\n"
     ]
    }
   ],
   "source": [
    "# trying to print a spark dataframe with print()\n",
    "# won't actually show the dataframe\n",
    "print(df_people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|NULL|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can \"print\" a Spark dataframe\n",
    "# by calling the show() method\n",
    "df_people.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of a Spark dataframe is defined by its `schema`. Just like SQL, a **schema** is a dictionary that assigns each column a specific data typing and additional options (e.g. can the column contain nulls?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the schema of a dataframe can be viewed with the\n",
    "# printSchema() method\n",
    "df_people.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark dataframes come with a `columns` attribute, which is just a python list of the column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'name']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the columns attribute;\n",
    "# note that this attribute is a just python list\n",
    "# so it will print as expected\n",
    "df_people.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Structure Fields and Editing Schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The schema of a Spark dataframe changed, which is useful if the `read.<file type>` functions don't behave like we want them to. In order to change the schema, we need to create a new `StructType` object. Again, because Spark dataframes have to be manipulated using Spark, the data types themselves must be loaded from a PySpark modueld `pyspark.sql.types`.\n",
    "\n",
    "In PySpark, a schema is just a **list of `StructField`** objects and a `StructField` object is specified by the following datum:\n",
    "\n",
    "1) The name of the column\n",
    "2) The type of the column\n",
    "3) Can the column contain nulls?\n",
    "\n",
    "Once this data is specified into a list of `StructField` objects, we can construct a `StructType` object which translates the datum into the necessary spark code to change the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, FloatType\n",
    "\n",
    "# specify a \"new\" schema for df_people where:\n",
    "# \"age\" = float (nulls are permitted)\n",
    "# \"name\" = string (nulls not permitted)\n",
    "# Note that the order of the fields must match the column order in the \n",
    "# underlying file\n",
    "new_fields = [\n",
    "    StructField(\"age\", FloatType(), nullable = True),\n",
    "    StructField(\"name\", StringType(), nullable = False),\n",
    "]\n",
    "\n",
    "# package the new fields into a new schema\n",
    "new_schema = StructType(fields = new_fields)\n",
    "\n",
    "# use this schema when reading in data files\n",
    "df_new_people = spark.read.json(\"../course_materials/Spark_Dataframes/sales_info.csv\", schema = new_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: float (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check that the new schema is being used\n",
    "df_new_people.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Column Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, we manipulate Spark dataframes by manipulating the columns. Columns for a Spark dataframe are implemented as Spark `Column` objects and must be interfaced with using Spark methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A column in a spark dataframe is a Spark Column object\n",
    "type(df_people[\"age\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `select()` Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `select()` method will collect together multiple Column objects and return them as a Spark dataframe. This allows us to then use dataframe methods on the columns by first using `select()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| age|\n",
      "+----+\n",
      "|NULL|\n",
      "|  30|\n",
      "|  19|\n",
      "+----+\n",
      "\n",
      "+----+\n",
      "| age|\n",
      "+----+\n",
      "|NULL|\n",
      "|  30|\n",
      "|  19|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select using SQL syntax; \n",
    "# we just pass the column name(s)\n",
    "df_people.select(\"age\").show()\n",
    "\n",
    "# select using \"dataframe\" syntax;\n",
    "# we specify the column object(s) to select\n",
    "df_people.select( df_people[\"age\"] ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Row Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark dataframes can be thought of as a list of Row objects. Accessing a specific value inside of a dataframe will typically require us to:\n",
    "\n",
    "1) Return the dataframe as a list of Row objects\n",
    "\n",
    "2) In the specific Row, access the specific column we need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `collect()` Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To return the dataframe as a list of rows, we use the `collect()` method. We can then acccess specific values by accessing the Row objects themselves. Note that a row object behaves like a \"named list\" so values within a Row object can be referenced by index (column number) or by name (column name)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=None, name='Michael'),\n",
       " Row(age=30, name='Andy'),\n",
       " Row(age=19, name='Justin')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return the dataframe as a list of Row objects\n",
    "df_people.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the age of Justin\n",
    "df_people.collect()[2][\"age\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 SQL Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark dataframes can also be temporarily stored in memory as SQL tables, allowing us to query them using SQL. To do this, we typically:\n",
    "\n",
    "1) Create a temporary view using the spark dataframe\n",
    "2) Execute a SQL query against the view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a temporary view using the dataframe\n",
    "df_people.createOrReplaceTempView(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# query the view using sql\n",
    "spark.sql(\"SELECT * FROM people WHERE age = 30\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udemy-pyspark-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
