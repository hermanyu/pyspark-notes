{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 Basic Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "# initiate spark session\n",
    "spark = SparkSession.builder.appName(\"Basic Operations\").getOrCreate()\n",
    "\n",
    "# load data\n",
    "df_sales = spark.read.csv(\"../course_materials/Spark_Dataframes/sales_info.csv\", header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Person: string (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Viewing Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `head()` Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `head()` method will return the top $n$ Row objects of a Spark dataframe, where $n$ can be specified by passing an argument. The default behavior will return only the 1st row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Company='GOOG', Person='Sam', Sales=200.0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `show()` Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `show()` method will print the top $n$ rows, but this time as a dataframe. Default behavior is to print the top 20 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|    Sam|  200|\n",
      "|   GOOG|Charlie|  120|\n",
      "|   GOOG|  Frank|  340|\n",
      "|   MSFT|   Tina|  600|\n",
      "|   MSFT|    Amy|  124|\n",
      "+-------+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the top 5 rows\n",
    "df_sales.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `orderBy()` Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `orderBy()` method sorts the rows of the dataframe according to the values of a given column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|     FB|   Carl|870.0|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.orderBy(\"Sales\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Subsetting Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `select()` Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `select()` method allows us to select and return a subset of columns as a new dataframe. The columns can be passed as a list of strings (SQL syntax) or using dataframe syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "| Person|Sales|\n",
      "+-------+-----+\n",
      "|    Sam|  200|\n",
      "|Charlie|  120|\n",
      "|  Frank|  340|\n",
      "|   Tina|  600|\n",
      "|    Amy|  124|\n",
      "|Vanessa|  243|\n",
      "|   Carl|  870|\n",
      "|  Sarah|  350|\n",
      "|   John|  250|\n",
      "|  Linda|  130|\n",
      "|   Mike|  750|\n",
      "|  Chris|  350|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select() using SQL Syntax\n",
    "df_sales.select(\"Person\", \"Sales\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "| Person|Sales|\n",
      "+-------+-----+\n",
      "|    Sam|  200|\n",
      "|Charlie|  120|\n",
      "|  Frank|  340|\n",
      "|   Tina|  600|\n",
      "|    Amy|  124|\n",
      "|Vanessa|  243|\n",
      "|   Carl|  870|\n",
      "|  Sarah|  350|\n",
      "|   John|  250|\n",
      "|  Linda|  130|\n",
      "|   Mike|  750|\n",
      "|  Chris|  350|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select() using dataframe syntax\n",
    "df_sales.select(df_sales[\"Person\"], df_sales[\"Sales\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe syntax is useful since we can manipulate the columns before they are returned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "| Person|Sales (in Hundreds)|\n",
      "+-------+-------------------+\n",
      "|    Sam|                2.0|\n",
      "|Charlie|                1.2|\n",
      "|  Frank|                3.4|\n",
      "|   Tina|                6.0|\n",
      "|    Amy|               1.24|\n",
      "|Vanessa|               2.43|\n",
      "|   Carl|                8.7|\n",
      "|  Sarah|                3.5|\n",
      "|   John|                2.5|\n",
      "|  Linda|                1.3|\n",
      "|   Mike|                7.5|\n",
      "|  Chris|                3.5|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.select(\n",
    "    df_sales[\"Person\"],\n",
    "    (df_sales[\"Sales\"] / 100).alias(\"Sales (in Hundreds)\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `filter()` Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `filter()` method allows us to subset the dataframe by rows, returning only the rows which satisfy some specified condition. This method accepts both SQL Syntax and dataframe syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|    Sam|  200|\n",
      "|   GOOG|  Frank|  340|\n",
      "|   MSFT|   Tina|  600|\n",
      "|   MSFT|Vanessa|  243|\n",
      "|     FB|   Carl|  870|\n",
      "|     FB|  Sarah|  350|\n",
      "|   APPL|   John|  250|\n",
      "|   APPL|   Mike|  750|\n",
      "|   APPL|  Chris|  350|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# subset to rows with sales >= 200; \n",
    "# SQL syntax\n",
    "df_sales.filter(\"Sales >= 200\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|    Sam|  200|\n",
      "|   GOOG|  Frank|  340|\n",
      "|   MSFT|   Tina|  600|\n",
      "|   MSFT|Vanessa|  243|\n",
      "|     FB|   Carl|  870|\n",
      "|     FB|  Sarah|  350|\n",
      "|   APPL|   John|  250|\n",
      "|   APPL|   Mike|  750|\n",
      "|   APPL|  Chris|  350|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# subset to rows with sales >= 200;\n",
    "# dataframe syntax\n",
    "df_sales.filter( df_sales[\"Sales\"] >= 200 ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Manipulating Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `withColumns()` Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, we will want take an existing column and manipulate it into a new column. This can be done with the `withColumns()` method by passing a dictionary of the form `{ new column : transformation }`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+-------------+-------------------+\n",
      "|Company| Person|Sales|    Full Name|Sales (in Hundreds)|\n",
      "+-------+-------+-----+-------------+-------------------+\n",
      "|   GOOG|    Sam|  200|    Sam Smith|                2.0|\n",
      "|   GOOG|Charlie|  120|Charlie Smith|                1.2|\n",
      "|   GOOG|  Frank|  340|  Frank Smith|                3.4|\n",
      "|   MSFT|   Tina|  600|   Tina Smith|                6.0|\n",
      "|   MSFT|    Amy|  124|    Amy Smith|               1.24|\n",
      "|   MSFT|Vanessa|  243|Vanessa Smith|               2.43|\n",
      "|     FB|   Carl|  870|   Carl Smith|                8.7|\n",
      "|     FB|  Sarah|  350|  Sarah Smith|                3.5|\n",
      "|   APPL|   John|  250|   John Smith|                2.5|\n",
      "|   APPL|  Linda|  130|  Linda Smith|                1.3|\n",
      "|   APPL|   Mike|  750|   Mike Smith|                7.5|\n",
      "|   APPL|  Chris|  350|  Chris Smith|                3.5|\n",
      "+-------+-------+-----+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat, lit\n",
    "\n",
    "df_sales.withColumns(\n",
    "    {\n",
    "        \"Full Name\" : concat(df_sales[\"Person\"], lit(\" Smith\")),\n",
    "        \"Sales (in Hundreds)\" : df_sales[\"Sales\"] / 100\n",
    "    }\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `withColumnsRenamed()` Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `withColumnsRenamed()` method is a simple method that renames columns. The method can be used by passing a dictionary of the form `{old column name : new column name}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+\n",
      "|Firm|   Name|Sales|\n",
      "+----+-------+-----+\n",
      "|GOOG|    Sam|  200|\n",
      "|GOOG|Charlie|  120|\n",
      "|GOOG|  Frank|  340|\n",
      "|MSFT|   Tina|  600|\n",
      "|MSFT|    Amy|  124|\n",
      "|MSFT|Vanessa|  243|\n",
      "|  FB|   Carl|  870|\n",
      "|  FB|  Sarah|  350|\n",
      "|APPL|   John|  250|\n",
      "|APPL|  Linda|  130|\n",
      "|APPL|   Mike|  750|\n",
      "|APPL|  Chris|  350|\n",
      "+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.withColumnsRenamed( {\"Person\" : \"Name\", \"Company\" : \"Firm\"} ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Aggregations and Group By's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `agg()` Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `agg()` method applies specified aggregation functions to specified columns. There are multiple syntatical ways to use the `agg()` method, but all of them more or less requires specifying a specific aggregation function to a specified column. The two most common approaches is:\n",
    "\n",
    "1) Passing a dictionary of the form `{ column : aggregation function }`. Note that this syntax uses column names as keys, which means two different aggregations *cannot* be applied to the same column.\n",
    "\n",
    "2) Passing expressions of the form `aggregation_function(column)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|sum(Sales)|\n",
      "+----------+\n",
      "|    4327.0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using dictionary syntax\n",
    "df_sales.agg({\"Sales\" : \"sum\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+---------+---------+-----------------+\n",
      "|Num. Rows|Total Sales|Min Sales|Max Sales|        Avg Sales|\n",
      "+---------+-----------+---------+---------+-----------------+\n",
      "|       12|     4327.0|    120.0|    870.0|360.5833333333333|\n",
      "+---------+-----------+---------+---------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using experssions\n",
    "from pyspark.sql import functions as psf\n",
    "\n",
    "df_sales.agg(\n",
    "    psf.count(\"Sales\").alias(\"Num. Rows\"),\n",
    "    psf.sum(\"Sales\").alias(\"Total Sales\"), \n",
    "    psf.min(\"Sales\").alias(\"Min Sales\"),\n",
    "    psf.max(\"Sales\").alias(\"Max Sales\"),\n",
    "    psf.mean(\"Sales\").alias(\"Avg Sales\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `groupBy()` Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `groupBy()` method allows us to restrict aggregation functions on specific groupings of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----------+---------+---------+-----------------+\n",
      "|Company|Num. Rows|Total Sales|Min Sales|Max Sales|        Avg Sales|\n",
      "+-------+---------+-----------+---------+---------+-----------------+\n",
      "|   APPL|        4|     1480.0|    130.0|    750.0|            370.0|\n",
      "|   GOOG|        3|      660.0|    120.0|    340.0|            220.0|\n",
      "|     FB|        2|     1220.0|    350.0|    870.0|            610.0|\n",
      "|   MSFT|        3|      967.0|    124.0|    600.0|322.3333333333333|\n",
      "+-------+---------+-----------+---------+---------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.groupBy(\"Company\").agg(\n",
    "    psf.count(\"Sales\").alias(\"Num. Rows\"),\n",
    "    psf.sum(\"Sales\").alias(\"Total Sales\"),\n",
    "    psf.min(\"Sales\").alias(\"Min Sales\"),\n",
    "    psf.max(\"Sales\").alias(\"Max Sales\"),\n",
    "    psf.mean(\"Sales\").alias(\"Avg Sales\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping PySpark Aggregation Functions in `select()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aggregation functions imported from `pyspark.sql.functions` can be called on columns of a dataframe and will return Colum objects. Column objects can be gathered by a `select()` statement and returned as a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|Total Sales|\n",
      "+-----------+\n",
      "|     4327.0|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using SQL syntax\n",
    "df_sales.select( psf.sum(\"Sales\").alias(\"Total Sales\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|Total Sales|\n",
      "+-----------+\n",
      "|     4327.0|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using dataframe syntax\n",
    "df_sales.select( psf.sum(df_sales[\"Sales\"]).alias(\"Total Sales\") ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Dealing With Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| NULL|\n",
      "|emp2| NULL| NULL|\n",
      "|emp3| NULL|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_nulls = spark.read.csv(\"../course_materials/Spark_Dataframes/ContainsNull.csv\", header = True, inferSchema = True)\n",
    "\n",
    "df_nulls.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping NAs with `na.drop()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `na.drop()` method will drop rows with missing values. The default behavior will drop any rows with *any* missing values in any columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop rows with any NULL values in any columns\n",
    "df_nulls.na.drop().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can control the exact behavior of `na.drop()` using 3 parameters:\r\n",
    "\r\n",
    "The how parameter changes whether to drop rows with any missing values or all missing values: how = \"any\" will drop rows with a missing value in any colum, while how = \"all\" will only drop rows that have missing values in all columns.\r\n",
    "The thresh parameter specifies how many non-null values must exist for a row to be kept; this will override the how parameter.\r\n",
    "The subset parameter specifies which columns to check for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| NULL|\n",
      "|emp3| NULL|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop only the rows that missing Name AND Sales\n",
    "df_nulls.na.drop(how = \"all\", subset = [\"Name\", \"Sales\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| NULL|\n",
      "|emp3| NULL|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# keep only rows that have at least 2 non-NULL value in any column\n",
    "df_nulls.na.drop(thresh = 2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing NAs with `na.fill()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `na.fill()` method will fill NULL values with a specified value. The default behavior will fill *all columns with a compatible type* as the fill value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|  0.0|\n",
      "|emp2| NULL|  0.0|\n",
      "|emp3| NULL|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fill all numeric columns with 0\n",
    "df_nulls.na.fill(0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+\n",
      "|  Id|   Name|Sales|\n",
      "+----+-------+-----+\n",
      "|emp1|   John| NULL|\n",
      "|emp2|unknown| NULL|\n",
      "|emp3|unknown|345.0|\n",
      "|emp4|  Cindy|456.0|\n",
      "+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fill all string columns with \"unknown\"\n",
    "df_nulls.na.fill(\"unknown\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exact behavior of `na.fill()` can be controlled by specifying the fill values to use to each specific column. This is done by passing a dictionary of the form `{ column : fill value}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+\n",
      "|  Id|   Name|Sales|\n",
      "+----+-------+-----+\n",
      "|emp1|   John|  0.0|\n",
      "|emp2|unknown|  0.0|\n",
      "|emp3|unknown|345.0|\n",
      "|emp4|  Cindy|456.0|\n",
      "+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_nulls.na.fill(\n",
    "    {\n",
    "        \"Name\" : \"unknown\", \n",
    "        \"Sales\" : 0 \n",
    "    }\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative syntax is to pass a `value` argument and a `subset` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+\n",
      "|  Id|   Name|Sales|\n",
      "+----+-------+-----+\n",
      "|emp1|   John| NULL|\n",
      "|emp2|unknown| NULL|\n",
      "|emp3|unknown|345.0|\n",
      "|emp4|  Cindy|456.0|\n",
      "+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_nulls.na.fill(value = \"unknown\", subset = [\"Name\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Imputing NAs With The Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common imputation method is **mean imputation** which fills NA values in a column with the mean of that column. The intuition is that the mean is the \"constant of best fit\", in the sense that the mean is the numerical value that minimizes the residual sum of squares. To perform mean imputation, we first have to compute the mean of column, extract that value, and use it as the argument in `na.fill()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400.5"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as psf\n",
    "\n",
    "# compute the mean value of the Sales column;\n",
    "# this will be returned as a 1x1 spark dataframe\n",
    "mean_sales = df_nulls.agg(psf.mean(df_nulls[\"Sales\"]).alias(\"mean_sales\"))\n",
    "\n",
    "# extract the value by accessing the Row object and\n",
    "# then the value in the Row object\n",
    "mean_sales = mean_sales.collect()[0][\"mean_sales\"]\n",
    "\n",
    "# mean_value should now be a python primitive\n",
    "mean_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|400.5|\n",
      "|emp2| NULL|400.5|\n",
      "|emp3| NULL|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# impute the Sales column with the mean\n",
    "df_nulls.na.fill({\"Sales\": mean_sales}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
